
from enum import Enum
from mcp.server.fastmcp import FastMCP
from mcp.types import TextContent, Prompt, PromptArgument, Resource  
import pandas as pd
import numpy as np
import json
from typing import Optional, List ,Dict
from pydantic import BaseModel

mcp = FastMCP(name="dsdb", host="127.0.0.1", port=8004)


# In-memory data store for loaded DataFrames
_dataframes = {}
_df_count = 0
_notes: list[str] = []

# Global DB engine and session (shared across all methods)
db_engine = None
db_session = None

def _next_df_name():
    global _df_count
    _df_count += 1
    return f"df_{_df_count}"


# --- DB Connection Args ---
class LoadCsvArgs(BaseModel):
    csv_path: str
    df_name: Optional[str] = None


# Remove LoadDbArgs and load_db tool, user will handle DB connection manually


class RunScriptArgs(BaseModel):
    script: str
    save_to_memory: Optional[List[str]] = None

class SqlQueryArgs(BaseModel):
    sql: str
    df_name: Optional[str] = None
# (DB connection and loading should be done manually by the user in a script, not as a tool)

@mcp.tool()
def run_sql_query(args: SqlQueryArgs) -> list:
    """Run a SQL query on the connected MySQL database and load the result into a DataFrame.
    The connection uses environment variables MYSQL_USER, MYSQL_PASSWORD, MYSQL_HOST, MYSQL_PORT, MYSQL_DB.
    sql: The SQL query to execute (e.g., SELECT ...)
    df_name: Optional DataFrame name (auto-assigned if not provided)
    """
    global _dataframes, _notes
    global db_engine
    sql = args.sql
    df_name = args.df_name or _next_df_name()
    try:
        if db_engine is None:
            return [TextContent(type="text", text="Database engine is not initialized. Startup connection failed or not configured.")]
        df = pd.read_sql_query(sql, db_engine)
        _dataframes[df_name] = df
        _notes.append(f"Loaded SQL query into dataframe '{df_name}' ({len(df)} rows)")
        return [TextContent(type="text", text=f"Loaded SQL query into dataframe '{df_name}' ({len(df)} rows)")]
    except Exception as e:
        error_msg = f"Error running SQL: {str(e)}"
        _notes.append(error_msg)
        return [TextContent(type="text", text=error_msg)]
# Tool: List all MySQL table names and their columns/attributes
class ListTablesArgs(BaseModel):
    pass

@mcp.tool()
def list_db_tables(args: ListTablesArgs = None) -> list:
    """List all table names in the connected MySQL database and their columns/attributes.
    Uses environment variables MYSQL_USER, MYSQL_PASSWORD, MYSQL_HOST, MYSQL_PORT, MYSQL_DB.
    """
    global db_engine
    from sqlalchemy import inspect
    try:
        if db_engine is None:
            return [TextContent(type="text", text="Database engine is not initialized. Startup connection failed or not configured.")]
        inspector = inspect(db_engine)
        tables = inspector.get_table_names()
        result = "=== DATABASE TABLES AND COLUMNS ===\n\n"
        for table in tables:
            columns = inspector.get_columns(table)
            col_str = ", ".join([f"{col['name']} ({col['type']})" for col in columns])
            result += f"Table: {table}\n  Columns: {col_str}\n\n"
        return [TextContent(type="text", text=result)]
    except Exception as e:
        return [TextContent(type="text", text=f"Error listing tables: {str(e)}")]


@mcp.tool()
def load_csv(args: LoadCsvArgs) -> list:
    """Load a local CSV file into a DataFrame."""
    global _dataframes, _notes
    csv_path = args.csv_path
    df_name = args.df_name
    if not df_name:
        df_name = _next_df_name()
    try:
        _dataframes[df_name] = pd.read_csv(csv_path, encoding='latin1')
        _notes.append(f"Successfully loaded CSV into dataframe '{df_name}'")
        return [TextContent(type="text", text=f"Successfully loaded CSV into dataframe '{df_name}'")]
    except Exception as e:
        error_msg = f"Error loading CSV: {str(e)}"
        _notes.append(error_msg)
        return [TextContent(type="text", text=error_msg)]


# (DB connection and loading should be done manually by the user in a script, not as a tool)
    
@mcp.tool()
def get_notes() -> list:
    """Return the notes generated by the data exploration server."""
    global _notes
    return [TextContent(type="text", text="\n".join(_notes))]

class PreviewDataFrameArgs(BaseModel):
    df_name: str
    n: int = 5  # Number of rows to preview

@mcp.tool()
def preview_dataframe(args: PreviewDataFrameArgs) -> list:
    """Preview the first n rows of a DataFrame in memory."""
    global _dataframes
    df_name = args.df_name
    n = args.n
    if df_name not in _dataframes:
        return [TextContent(type="text", text=f"DataFrame '{df_name}' not found. Available: {list(_dataframes.keys())}")]
    df = _dataframes[df_name]
    preview = df.head(n).to_string(index=False)
    return [TextContent(type="text", text=f"Preview of '{df_name}' (first {n} rows):\n{preview}")]


@mcp.tool()
def run_script(args: RunScriptArgs) -> list:
    """Execute a Python script for data analytics tasks.
    Do list_dataframes before using this tool to ensure you know the available DataFrames and their columns.
    This tool allows you to run scripts that can analyze, process, and visualize data using pandas."""
    global _dataframes, _notes
    script = args.script
    save_to_memory = args.save_to_memory
    
    # Basic validation
    if 'pd.read_csv(' in script:
        return [TextContent(type="text", text="ERROR: Use load_csv tool instead of pd.read_csv()")]
    
    if any(pattern in script for pattern in ['import matplotlib', 'plt.show(', 'plt.figure(']):
        return [TextContent(type="text", text="ERROR: Use DataFrame.plot() methods instead of matplotlib")]
    
    # Execute script
    local_dict = {**_dataframes}
    
    try:
        import sys
        from io import StringIO
        
        # Capture output
        old_stdout = sys.stdout
        sys.stdout = captured_output = StringIO()
        
        # Execute
        exec(script, {'pd': pd, 'np': np}, local_dict)
        
        # Get output
        output = captured_output.getvalue()
        
    except Exception as e:
        output = f"Error: {str(e)}"
    finally:
        sys.stdout = old_stdout
    
    # Save to memory if requested
    if save_to_memory:
        for df_name in save_to_memory:
            if df_name in local_dict:
                _dataframes[df_name] = local_dict[df_name]
    
    if not output.strip():
        output = "Script executed successfully (no output)"
    
    _notes.append(f"Script executed: {output[:100]}...")
    return [TextContent(type="text", text=output)]

@mcp.tool()
def list_dataframes() -> list:
    """List all DataFrames currently loaded in memory."""
    global _dataframes
    if not _dataframes:
        return [TextContent(type="text", text="No DataFrames loaded. Use load_csv tool first.")]
    
    result = "=== DATAFRAMES IN MEMORY ===\n\n"
    for name, df in _dataframes.items():
        result += f"DataFrame: {name}\n"
        result += f"  Shape: {df.shape}\n"
        result += f"  Columns: {list(df.columns)}\n\n"
    
    result += f"Use these names in scripts: {', '.join(_dataframes.keys())}"
    return [TextContent(type="text", text=result)]



# @mcp.tool()
# def create_simple_chart(df_name: str, chart_type: str, column: str = None) -> list:
#     """Create simple chart data for visualization."""
#     global _dataframes
    
#     if df_name not in _dataframes:
#         return [TextContent(type="text", text=f"DataFrame '{df_name}' not found")]
    
#     df = _dataframes[df_name]
    
#     try:
#         if chart_type == "histogram" and column:
#             values = df[column].dropna().tolist()
#             chart_data = {
#                 "type": "histogram",
#                 "data": values,
#                 "title": f"Distribution of {column}",
#                 "column": column
#             }
#         elif chart_type == "line" and column:
#             chart_data = {
#                 "type": "line", 
#                 "data": df[column].tolist(),
#                 "title": f"{column} Over Time",
#                 "column": column
#             }
#         else:
#             return [TextContent(type="text", text="Supported: histogram, line")]
        
#         return [TextContent(type="text", text=json.dumps(chart_data, indent=2))]
    
#     except Exception as e:
#         return [TextContent(type="text", text=f"Error creating chart: {str(e)}")]

@mcp.tool()

# --- Improved modular visualization tool ---
class CreateVisualizationArgs(BaseModel):
    df_name: str
    plot_type: str
    x: Optional[str] = None
    y: Optional[str] = None
    column: Optional[str] = None
    title: Optional[str] = None
    bins: int = 20
    max_points: int = 100

def _extract_plot_data(df, plot_type, x=None, y=None, column=None, title=None, bins=20, max_points=100):
    if plot_type == "stacked_bar" and x and y:
        # y can be a list of columns to stack
        y_cols = y if isinstance(y, list) else [y]
        data = df[[x] + y_cols].dropna()
        grouped = data.groupby(x)[y_cols].sum().head(max_points)
        bars = {col: grouped[col].tolist() for col in y_cols}
        x_labels = grouped.index.tolist()
        return {
            "type": "stacked_bar",
            "x": x_labels,
            "bars": bars,
            "y_cols": y_cols,
            "title": title or f"Stacked Bar of {y_cols} by {x}"
        }
    elif plot_type == "heatmap" and x and y:
        # Check if there's a third column to aggregate, otherwise count
        if column:
            # Use the specified column as values to aggregate
            pivot = pd.pivot_table(df, index=y, columns=x, values=column, aggfunc="sum", fill_value=0)
            title_default = f"Heatmap of {column} by {y} vs {x}"
        else:
            # Fallback to counting occurrences
            pivot = pd.pivot_table(df, index=y, columns=x, aggfunc="size", fill_value=0)
            title_default = f"Heatmap count of {y} vs {x}"
        
        return {
            "type": "heatmap",
            "x": list(pivot.columns),
            "y": list(pivot.index),
            "z": pivot.values.tolist(),
            "title": title or title_default,
            "aggregated_column": column if column else None
        }

    elif plot_type == "box" or plot_type == "boxplot":
        # y can be a single column or list of columns
        y_cols = y if isinstance(y, list) else [y]
        box_data = {}
        for col in y_cols:
            if col in df.columns:
                box_data[col] = df[col].dropna().tolist()
        return {
            "type": "boxplot",
            "columns": y_cols,
            "data": box_data,
            "title": title or f"Boxplot of {y_cols}"
        }
    elif plot_type == "histogram" and column:
        values = df[column].dropna().tolist()
        counts, bin_edges = np.histogram(values, bins=bins)
        return {
            "type": "histogram",
            "bins": [
                {"range": [float(bin_edges[i]), float(bin_edges[i+1])], "count": int(counts[i])}
                for i in range(len(counts))
            ],
            "title": title or f"Distribution of {column}",
            "column": column
        }
    elif plot_type == "line":
        # Support both (x, y) and (column) usage
        if x and y:
            data = df[[x, y]].dropna()
            x_vals = data[x].tolist()
            y_vals = data[y].tolist()
            if len(x_vals) > max_points:
                step = max(1, len(x_vals) // max_points)
                x_vals = x_vals[::step]
                y_vals = y_vals[::step]
            return {
                "type": "line",
                "points": [{"x": str(xv), "y": float(yv)} for xv, yv in zip(x_vals, y_vals)],
                "title": title or f"{y} vs {x}",
                "x": x,
                "y": y
            }
        elif column:
            y_data = df[column].dropna()
            x_data = y_data.index.tolist()
            if len(x_data) > max_points:
                step = max(1, len(x_data) // max_points)
                x_data = x_data[::step]
                y_data = y_data.iloc[::step]
            return {
                "type": "line",
                "points": [{"x": str(x), "y": float(y)} for x, y in zip(x_data, y_data)],
                "title": title or f"{column} Over Index",
                "column": column
            }
        else:
            return {"error": "For line plot, provide either (column) or (x, y)"}
    elif plot_type == "bar" and x and y:
        grouped = df.groupby(x)[y].sum().sort_values(ascending=False)
        grouped = grouped.head(max_points)
        return {
            "type": "bar",
            "bars": [{"label": str(idx), "value": float(val)} for idx, val in grouped.items()],
            "title": title or f"{y} by {x}",
            "x": x,
            "y": y
        }
    elif plot_type == "pie" and column:
        value_counts = df[column].value_counts().head(max_points)
        return {
            "type": "pie",
            "slices": [
                {"label": str(idx), "value": int(val)} for idx, val in value_counts.items()
            ],
            "title": title or f"Pie chart of {column}",
            "column": column
        }
    elif plot_type == "area":
        # Area chart: support multi-series by a category column (e.g., PRODUCTLINE)
        if x and y:
            # If 'column' is provided, treat it as the series key (e.g., PRODUCTLINE)
            if column and column in df.columns:
                # Group by x and column, sum y
                grouped = df[[x, column, y]].dropna().groupby([x, column])[y].sum().reset_index()
                # Build a dict: {series_name: [{x, y}, ...]}
                area_series = {}
                for series_name in grouped[column].unique():
                    series_data = grouped[grouped[column] == series_name]
                    x_vals = series_data[x].tolist()
                    y_vals = series_data[y].tolist()
                    # Downsample if needed
                    if len(x_vals) > max_points:
                        step = max(1, len(x_vals) // max_points)
                        x_vals = x_vals[::step]
                        y_vals = y_vals[::step]
                    area_series[str(series_name)] = [
                        {"x": str(xv), "y": float(yv)} for xv, yv in zip(x_vals, y_vals)
                    ]
                return {
                    "type": "area",
                    "series": area_series,
                    "title": title or f"Area chart of {y} by {x} per {column}",
                    "x": x,
                    "y": y,
                    "series_column": column
                }
            else:
                # Fallback: treat y as a list of columns (legacy behavior)
                y_cols = y if isinstance(y, list) else [y]
                data = df[[x] + y_cols].dropna()
                x_vals = data[x].tolist()
                area_series = {}
                for y_col in y_cols:
                    y_vals = data[y_col].tolist()
                    if len(x_vals) > max_points:
                        step = max(1, len(x_vals) // max_points)
                        x_vals_ds = x_vals[::step]
                        y_vals_ds = y_vals[::step]
                    else:
                        x_vals_ds = x_vals
                        y_vals_ds = y_vals
                    area_series[y_col] = [{"x": str(xv), "y": float(yv)} for xv, yv in zip(x_vals_ds, y_vals_ds)]
                return {
                    "type": "area",
                    "series": area_series,
                    "title": title or f"Area chart of {y} by {x}",
                    "x": x,
                    "y": y
                }
        else:
            return {"error": "For area chart, provide x and y (y can be a list), and optionally column for series"}
    elif plot_type == "scatter" and x and y:
        data = df[[x, y]].dropna()
        x_vals = data[x].tolist()
        y_vals = data[y].tolist()
        if len(x_vals) > max_points:
            step = max(1, len(x_vals) // max_points)
            x_vals = x_vals[::step]
            y_vals = y_vals[::step]
        return {
            "type": "scatter",
            "points": [{"x": str(xv), "y": float(yv)} for xv, yv in zip(x_vals, y_vals)],
            "title": title or f"Scatterplot of {y} vs {x}",
            "x": x,
            "y": y
        }
    else:
        return {"error": "Supported: histogram (column), line (column or x/y), bar (x, y), stacked_bar (x, y[list]), pie (column), area (x, y), scatter (x, y), heatmap (x, y), boxplot (y)"}


@mcp.tool()
def create_visualization(args: CreateVisualizationArgs) -> list:
    """Create visualization data for React frontend.
    This tool extracts data for various chart types from a DataFrame.
    Do list all supported chart types using list_supported_chart_types tool.
    Do list_dataframes tool before using this tool to ensure you know the available DataFrames and their columns.
    """
    global _dataframes
    df_name = args.df_name
    if df_name not in _dataframes:
        return [TextContent(type="text", text=f"DataFrame '{df_name}' not found. Available: {list(_dataframes.keys())}")]
    df = _dataframes[df_name]
    # Accept x and y as comma-separated string or list
    x = args.x
    y = args.y
    if isinstance(x, str) and "," in x:
        x = [col.strip() for col in x.split(",")]
    if isinstance(y, str) and "," in y:
        y = [col.strip() for col in y.split(",")]
    plot_type = args.plot_type.lower() if isinstance(args.plot_type, str) else args.plot_type

    plot_data = _extract_plot_data(
        df,
        plot_type,
        x=x,
        y=y,
        column=args.column,
        title=args.title,
        bins=args.bins,
        max_points=args.max_points
    )
    return [TextContent(type="text", text=json.dumps(plot_data, indent=2))]

# Tool: List supported chart types
@mcp.tool()
def list_supported_chart_types() -> list:
    """List all supported chart types for visualization."""
    chart_types = [
        "histogram",
        "line",
        "bar",
        "stacked_bar",
        "pie",
        "area",
        "scatter",
        "heatmap",
        "boxplot"
    ]
    return [TextContent(type="text", text="Supported chart types: " + ", ".join(chart_types))]

### Data Exploration Tools Description & Schema
_dataframes: Dict[str, pd.DataFrame] = {}

### Prompt templates
class DataExplorationPrompts(str, Enum):
    EXPLORE_DATA = "explore-data"

class PromptArgs(str, Enum):
    CSV_PATH = "csv_path"
    TOPIC = "topic"

# PROMPT_TEMPLATE = """
# You are a professional Data Scientist tasked with performing exploratory data analysis on a dataset. Your goal is to provide insightful analysis while ensuring stability and manageable result sizes.

# First, load the CSV file from the following path:

# <csv_path>
# {csv_path}
# </csv_path>

# Your analysis should focus on the following topic:

# <analysis_topic>
# {topic}
# </analysis_topic>

# You have access to the following tools for your analysis:
# 1. load_csv: Use this to load the CSV file.
# 2. list_dataframes: Use this to see all available DataFrames and their columns.
# 3. list_supported_chart_types: Use this to see all supported chart types for visualization.
# 4. run_script: Use this to execute Python scripts on the MCP server.
# 5. create_visualization: Use this to create charts from DataFrames.

# Important: Before using run_script or create_visualization, always call list_dataframes to discover available DataFrames and their columns, and call list_supported_chart_types to discover supported chart types.

# Please follow these steps carefully:

# 1. Load the CSV file using the load_csv tool.

# 2. Call list_dataframes to see all available DataFrames and their columns.

# 3. Call list_supported_chart_types to see all supported chart types for visualization.

# 4. Explore the dataset. Provide a brief summary of its structure, including the number of rows, columns, and data types. Wrap your exploration process in <dataset_exploration> tags, including:
#    - List of key statistics about the dataset
#    - Potential challenges you foresee in analyzing this data

# 5. Wrap your thought process in <analysis_planning> tags:
#    Analyze the dataset size and complexity:
#    - How many rows and columns does it have?
#    - Are there any potential computational challenges based on the data types or volume?
#    - What kind of questions would be appropriate given the dataset's characteristics and the analysis topic?
#    - How can we ensure that our questions won't result in excessively large outputs?

#    Based on this analysis:
#    - List 10 potential questions related to the analysis topic
#    - Evaluate each question against the following criteria:
#      * Directly related to the analysis topic
#      * Can be answered with reasonable computational effort
#      * Will produce manageable result sizes
#      * Provides meaningful insights into the data
#    - Select the top 5 questions that best meet all criteria

# 6. List the 5 questions you've selected, ensuring they meet the criteria outlined above.

# 7. For each question, follow these steps:
#    a. Wrap your thought process in <analysis_planning> tags:
#       - How can I structure the Python script to efficiently answer this question?
#       - What data preprocessing steps are necessary?
#       - How can I limit the output size to ensure stability?
#       - What type of visualization would best represent the results?
#       - Outline the main steps the script will follow
   
#    b. Write a Python script to answer the question. Include comments explaining your approach and any measures taken to limit output size.
   
#    c. Use the run_script tool to execute your Python script on the MCP server.
   

# 8. After completing the analysis for all 5 questions, provide a brief summary of your findings and any overarching insights gained from the data.

# Remember to prioritize stability and manageability in your analysis. If at any point you encounter potential issues with large result sets, adjust your approach accordingly.

# Please begin your analysis by loading the CSV file and providing an initial exploration of the dataset.
# """

PROMPT_TEMPLATE = """
You are a professional Data Scientist. Perform exploratory data analysis on the dataset at this path:

<csv_path>
{csv_path}
</csv_path>

Focus your analysis on: **{topic}**

## Your Task:
1. Load and explore the dataset structure
2. Identify 3-5 key insights related to {topic}
3. Create appropriate visualizations to support your findings
4. Provide a concise summary of your analysis

## Important Guidelines:
- Always call `list_dataframes` before using `run_script` or `create_visualization`
- Keep individual script outputs manageable (limit large results)
- Focus on the most relevant insights for the given topic
- Use visualizations to enhance understanding

Begin by loading the CSV file and exploring its basic structure.
"""

# --- Prompt template for database chat/analysis ---
DB_PROMPT_TEMPLATE = """
You are a professional Data Scientist and SQL expert. You have access to a MySQL database connection and a set of tools for querying, exploring, and visualizing data.

## Your Task:
1. Explore the database schema (tables and columns) using the `list_db_tables` tool.
2. For any analysis, always:
   - Use `run_sql_query` to run SQL queries and load results into DataFrames.
   - Use `list_dataframes` to see available DataFrames and their columns.
   - Use `create_visualization` to generate charts from DataFrames.
3. When asked a question, follow this process:
   - Identify which tables and columns are relevant.
   - Write a SQL query to answer the question (limit results if needed).
   - Load the query result into a DataFrame using `run_sql_query`.
   - Analyze the DataFrame and, if appropriate, create a visualization.
   - Summarize your findings concisely.

## Important Guidelines:
- Always use the tools (`list_db_tables`, `run_sql_query`, `list_dataframes`, `create_visualization`) for all data access and analysis.
- Never access the database directly except through the provided tools.
- Limit query result sizes to avoid large outputs (e.g., use LIMIT or aggregation).
- If you are unsure about the schema, call `list_db_tables` first.
- Provide SQL queries that are safe and efficient.

Begin by exploring the database schema and proceed step by step for each user question or analysis request.
"""

#    d. Render the results returned by the run_script tool as a chart using plotly.js (prefer loading from cdnjs.cloudflare.com). Do not use react or recharts, and do not read the original CSV file directly. Provide the plotly.js code to generate the chart.


class DataExplorationTools(str, Enum):



    LOAD_CSV = "load_csv"
    RUN_SCRIPT = "run_script"

LOAD_CSV_TOOL_DESCRIPTION = """
Load CSV File Tool

Purpose:
Load a local CSV file into a DataFrame.

Usage Notes:
    •	If a df_name is not provided, the tool will automatically assign names sequentially as df_1, df_2, and so on.
    •	NEVER load CSV files in run_script - use this tool exclusively for CSV loading.
"""

RUN_SCRIPT_TOOL_DESCRIPTION = """
Python Script Execution Tool

Purpose:
Execute Python scripts for specific data analytics tasks.

Allowed Actions
    1.	Print Results: Output will be displayed as the script's stdout.
    2.	[Optional] Save DataFrames: Store DataFrames in memory for future use by specifying a save_to_memory name.
    3.	Data Analysis: Calculate statistics, aggregations, correlations, etc.
    4.	Data Processing: Clean, transform, filter, group data.
    5.	DataFrame Charts: Use DataFrame.plot() methods - charts will auto-display in frontend!

Prohibited Actions
    1.	Overwriting Original DataFrames: Do not modify existing DataFrames to preserve their integrity for future tasks.
    2.	Loading CSV files: Use load_csv tool instead. Scripts should only work with DataFrames already in memory.
    3.	Direct matplotlib usage: Use DataFrame.plot() methods instead of plt.show(), plt.figure(), etc.
    4.	Saving Files: Scripts should not save files, only print results and create charts.

Available DataFrames: df_1, df_2, etc. (loaded via load_csv tool)

Chart Examples (Auto-Display):
- df.plot(kind='bar', title='Bar Chart') - Bar chart
- df.plot(kind='line', x='date', y='value') - Line chart
- df['column'].value_counts().plot(kind='pie') - Pie chart
- df.plot(kind='scatter', x='col1', y='col2') - Scatter plot
- df.plot(kind='hist', bins=20) - Histogram

Best Practices:
- Use df.plot() methods for charts that will auto-display
- Print summary statistics and insights
- Combine charts with printed analysis for complete insights
- Focus on meaningful visualizations that tell a story
"""

@mcp.prompt()
def explore_data_prompt():
    """A prompt to explore a CSV dataset as a data scientist"""
    print("Registering explore_data_prompt")
    return DB_PROMPT_TEMPLATE


# Resource: DataFrame schema and preview
@mcp.resource("data-exploration://schema", mime_type="application/json")
def schema_resource():
    """All DataFrames in memory with columns, dtypes, shape, and preview rows."""
    print("[RESOURCE] schema_resource called")
    global _dataframes
    schema = {}
    for name, df in _dataframes.items():
        schema[name] = {
            "columns": list(df.columns),
            "dtypes": {col: str(df.dtypes[col]) for col in df.columns},
            "shape": list(df.shape),
            "preview": df.head(5).to_dict(orient="records")
        }
    return schema
# Resource: Supported chart types
@mcp.resource("data-exploration://chart-types", mime_type="application/json")
def chart_types_resource():
    """List of supported chart types for visualization."""
    print("[RESOURCE] chart_types_resource called")
    return [
        "histogram",
        "line",
        "bar",
        "pie",
        "area",
        "scatter"
    ]
# Keep notes resource as well
@mcp.resource("data-exploration://notes", mime_type="text/plain")
def notes_resource():
    """Notes generated by the data exploration server."""
    print("[RESOURCE] notes_resource called")
    global _notes
    return "\n".join(_notes) if _notes else "No notes yet. Use load_csv or run_script to generate notes."

if __name__ == "__main__":
    # --- Connect to MySQL and share the engine globally ---
    try:
        from sqlalchemy import create_engine
        from dotenv import load_dotenv
        import os
        load_dotenv()  # Load environment variables from .env file
        MYSQL_USER = os.getenv("MYSQL_USER")
        MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD")
        MYSQL_HOST = os.getenv("MYSQL_HOST")
        MYSQL_PORT = os.getenv("MYSQL_PORT")
        MYSQL_DB = os.getenv("MYSQL_DB")
        db_url = f"mysql+pymysql://{MYSQL_USER}:{MYSQL_PASSWORD}@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}"
        db_engine = create_engine(db_url)
        print("[Startup] MySQL engine initialized and shared globally.")
    except Exception as e:
        db_engine = None
        print(f"[Startup] MySQL engine initialization failed: {e}")

    mcp.run(transport="streamable-http")
